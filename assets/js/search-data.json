{
  
    
        "post0": {
            "title": "Softmax Regression",
            "content": "Import packages . import pandas as pd import torch from torch import Tensor from keras.utils.np_utils import to_categorical import time . |████████████████████████████████| 727kB 26.2MB/s |████████████████████████████████| 1.2MB 20.5MB/s |████████████████████████████████| 51kB 6.6MB/s |████████████████████████████████| 204kB 41.2MB/s |████████████████████████████████| 61kB 7.1MB/s |████████████████████████████████| 61kB 7.4MB/s Mounted at /content/gdrive . path = untar_data(URLs.MNIST) . Path.Base_PATH = path . path.ls() . (#2) [Path(&#39;/root/.fastai/data/mnist_png/training&#39;),Path(&#39;/root/.fastai/data/mnist_png/testing&#39;)] . &#39;&#39;&#39; Important info: #training images = 60,000 Each of the categories having roughly equal distribution: train_y.unique(return_counts=True) &#39;&#39;&#39; train_images_list = get_image_files(path/&#39;training&#39;) train_x_list = [tensor(Image.open(img_path)) for img_path in train_images_list] train_y_list = [int(img_path.parent.name) for img_path in train_images_list] train_x = (torch.stack(train_x_list).float()/255).view(-1,28*28) train_y = tensor(train_y_list).view(-1,1) train_x.shape, train_y.shape . (torch.Size([60000, 784]), torch.Size([60000, 1])) . train_dset = list(zip(train_x, train_y)) . &#39;&#39;&#39; Important info: #validation images = 10,000 Each of the categories having roughly equal distribution: valid_y.unique(return_counts=True) &#39;&#39;&#39; valid_images_list = get_image_files(path/&#39;testing&#39;) valid_x_list = [tensor(Image.open(img_path)) for img_path in valid_images_list] valid_y_list = [int(img_path.parent.name) for img_path in valid_images_list] valid_x = (torch.stack(valid_x_list).float()/255).view(-1,28*28) valid_y = tensor(valid_y_list).view(-1,1) valid_x.shape, valid_y.shape . (torch.Size([10000, 784]), torch.Size([10000, 1])) . valid_dset = list(zip(valid_x, valid_y)) . #learn = cnn_learner(dls, resnet18, pretrained=False, # loss_func=F.cross_entropy, metrics=accuracy, n_out=10) #learn.fit_one_cycle(1, 0.1) . Softmax Regression . . We are going to start with some input data in the above picture our inputs have the dimension of (m x 4), that&#39;s why there are 4 nodes. . Each feature/columns of the input is mapped to every node in the hidden layer making it a fully connected network. The nodes are floating point numbers referred to as logits and are the result of multiplying every input by a set of weights that are unique to the node and adding a bias. . The output of X@w + b which is the logits then get transformed by being passed through a activation function usually a sigmoid function that takes a input and produces and output between 0 and 1, which represents a probability that that row of data has a particular label. . In the final layer the node in the output layer with the highest probability is chosen and that is then the predicted class . One iteration of the model training process for a network like this can be split up into two sections - the forward pass and backward pass. . Forward Pass: Starts at the input layer and ends with the output layer and predicted class. The performance of the forward pass is evaluated through the calculation of a loss function that tells the model how well or bad it is doing. . Backward Pass: Once we have the training loss, we use that to go back through the network and make adjustments to the parameters of all the hidden layers in order to reduce the loss and improve the prediction this optimization step is done with Gradient Descent. . Steps: . Initialization | Predict | Loss | Calculate gradient | Update Parameters | Back to step 2 | Stop | Read The Data . X = torch.tensor(train_x, dtype=torch.float32) y = torch.tensor(train_y) . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor). &#34;&#34;&#34;Entry point for launching an IPython kernel. /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor). . y.shape, y.unique() . (torch.Size([60000, 1]), tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])) . One-Hot Encode Class Labels: . $% &lt;![CDATA[ begin{equation*} y = begin{bmatrix} 1 2 1 0 2 0 end{bmatrix} implies begin{bmatrix} 0 &amp; 1 &amp; 0 0 &amp; 0 &amp; 1 0 &amp; 1 &amp; 0 1 &amp; 0 &amp; 0 0 &amp; 0 &amp; 1 1 &amp; 0 &amp; 0 end{bmatrix} end{equation*} %]]&gt;$ . This is an important technique for preprocessing certain categorical features in your training data as well. For this use case it just makes some computations easier. . Get the number of unique values in y. Create a tensor of zeros with shape (n_training_samples, n_classes). Use the scatter method to replace zeros in one_hot where there should be a 1 to represent that a given row is of the specific iris type. The first argument is the axis along which to work, which in this case is 1 for the second dimension (across rows). The second argument supplies indices that represent the column in each row that will get a 1. The trick here is that we already encoded the Iris labels with integers 0, 1, and 2, which happen to also specify valid columns. The last argument is the value(s) we want to impute, which in this case is just 1 for every row. . def one_hot_encode(vector): classes = len(vector.unique()) one_hot = torch.zeros((vector.shape[0], classes)).type(torch.LongTensor) return one_hot.scatter(1,y.type(torch.LongTensor), 1) y_one_hot = one_hot_encode(y) . def one_hot(y, c): # y--&gt; label/ground truth. # c--&gt; Number of classes. # A zero matrix of size (m, c) y_hot = torch.zeros((len(y), c)) # Putting 1 for column where the label is, # Using multidimensional indexing. y_hot[torch.arange(len(y)), y] = 1 return y_hot . y_one_hot2 = one_hot(y, 10) . y_one_hot.shape, y_one_hot2.shape . (torch.Size([60000, 10]), torch.Size([60000, 10])) . Initialize Model Parameters . Each example will be represented by a fixed length vector (1D tensor). Since each example in the raw dataset is 28x28 pixel image, we flatten the image and will treat it as a 1D vector with lenght 784. . Since we are doing sofmax regression which is a multiclass classification our output will have 10 categories. Therefore the weights will be 784 x 10 matrix and the biases will be 1 x 10 row vector. . We initialize our weights w with Guassian noise and out biases to 0s. . weights = torch.randn((784, 10)) bias = torch.randn(10) weights.shape, bias.shape . (torch.Size([784, 10]), torch.Size([10])) . Define the Softmax Operations . Before we implement the softmax model, lets highlight how the sum operator can work along specific dimensions of a tensor: . #X.sum(), X.sum(0), X.sum(1), X.sum(0, keepdim=True), X.sum(1,keepdim=True) . . Time to implement the softmax operation, which consists of 3 steps: . Exponentiate each term | Sum over each row to get the normalization constant for each example | Divide each row by its normalization constant, ensuring that the result sums to 1. | $ mathrm{softmax}( mathbf{X})_{ij} = frac{ exp( mathbf{X}_{ij})}{ sum_k exp( mathbf{X}_{ik})}.$ . The denominator is the normalization constant aka the parition function. . def softmax(X): exps = torch.exp(X) partition = exps.sum(1, keepdim=True) return exps / partition . Defining The Model . Linear Model The first step of the model requires multiplying each m rows (training examples) of the dataset by a weight matrix with n rows and k columns, where n is the number of columns in your dataset (features) and k is the number of unique classes/labels you’ll want to predict. In addition to the weight matrix we’ll add a bias term to each column of the result. . These operations are implementing this equation: . Z . X W + b Where, . $% &lt;![CDATA[ begin{equation*} X = begin{bmatrix} x_{0,0} &amp; x_{0,1} &amp; cdots &amp; x_{0,n} x_{1,0} &amp; x_{1,1} &amp; cdots &amp; x_{1,n} vdots &amp; vdots &amp; ddots &amp; vdots x_{m,0} &amp; x_{m,1} &amp; cdots &amp; x_{m,n} end{bmatrix} : :W = begin{bmatrix} w_{0,0} &amp; w_{0,1} &amp; cdots &amp; w_{0,k} w_{1,0} &amp; w_{1,1} &amp; cdots &amp; w_{1,k} vdots &amp; vdots &amp; cdots &amp; vdots w_{n,0} &amp; w_{n,1} &amp; cdots &amp; w_{n,k} end{bmatrix} end{equation*} %]]&gt;$ . $% &lt;![CDATA[ begin{equation*} b = begin{bmatrix} b_{0} &amp; b_{1} &amp; cdots &amp; b_{k} end{bmatrix} : :Z = begin{bmatrix} z_{0,0} &amp; z_{0,1} &amp; cdots &amp; z_{0,k} z_{1,0} &amp; z_{1,1} &amp; cdots &amp; z_{1,k} vdots &amp; vdots &amp; ddots &amp; vdots z_{m,0} &amp; z_{m,1} &amp; cdots &amp; z_{m,k} end{bmatrix} end{equation*} %]]&gt;$ . Now that we have defined the sofmat operation, we can implement the softmax regression model: . In logistic regression terms, this resulting Z is a matrix of logits, where each z i , j is the logit for the j t h label of the i t h training example. . To put these things in terms of the Iris dataset, our n will be 4 for the sepal length, sepal width, petal length, and petal width features. k is 3 for for the Setosa, Versicolour, and Virginica classes. And although there’s 150 total observations/rows, our training size m will be (somewhat arbitrarily) 120, or 80% of the data. . def forward_pass(x, w, b): Z = torch.matmul(x,w) + bias A = softmax(Z) return A A = forward_pass(X, weights, bias) . A.shape . torch.Size([60000, 10]) . A.shape, X.shape, y_one_hot.shape . (torch.Size([60000, 10]), torch.Size([60000, 784]), torch.Size([60000, 10])) . Calculate the Gradients . In order to build a model that can make accuratte predictions, we need to find out what impact each paramater has on the average loss. With that information we can iteratively adjust the parameters so that the loss gets smaller and smaller i.e. we want to find the parameters that minimize the cost function, the algorithm for doing this is called gradient descent. . We start by initializing ´weights´ and ´biases´ randomly since these random values will perfom very poorly we need to perform the following steps at the end of each iteration: . Get the gradient of each model parameter. . The gradient is the partial derivative of the parameter at its current value with respect to the cost function at it&#39;s current value. | The algorithm for computing the gradients is called backpropagation | . | Update each model parameter in the opposite direction of its gradient. . The gradient $g$ is a measure of the impact that a parameter has on the cost function $g&gt;0$ implies that the cost/loss is increasing and $g&lt;0$ means that it&#39;s decreasing. | The learning rate $ alpha$ is a scalar value that we multiply the gradient with during our update step to determine the step size in our parameter update. | The goal is to get as close to the global minimum of the cost function as possible: | . | . The matrix of gradients of all the weights: . $% &lt;![CDATA[ nabla (W) = begin{bmatrix} nabla (w_{0,0}) &amp; nabla (w_{0,1}) &amp; cdots &amp; nabla (w_{0,k}) nabla (w_{1,0}) &amp; nabla (w_{1,1}) &amp; cdots &amp; nabla (w_{1,k}) vdots &amp; vdots &amp; cdots &amp; vdots nabla (w_{n,0}) &amp; nabla (w_{n,1}) &amp; cdots &amp; nabla (w_{n,k}) end{bmatrix} : :and %]]&gt;$ . Vector of the gradients of all the biases: . $% &lt;![CDATA[ nabla (b) = begin{bmatrix} nabla (b_{0}) &amp; nabla (b_{1}) &amp; cdots &amp; nabla (b_{k}) end{bmatrix} %]]&gt;$ . Where, . $ nabla (w_{f,l}) = [ frac{1}{m} sum_{i=0}^{m} left( x_{i,f} * (y_{i, l} - a_{i, l}) right)] + [2 lambda * w_{f,l}] : :and$ . $ nabla (b_{l}) = frac{1}{m} sum_{i=0}^{m} left( y_{i, l} - a_{i, l} right)$ . $f$ is the feature index, $l$ is the class label index, and $i$ is the specific simple/observation index. Note: that $y$ in this equation is one-hot encoded and of shape (m,k) were k is the number of classes. . The first term $ nabla (w_{f,l})$ is the gradient with respect to the cross entropy function and the second term is the gradient with respect to the L2 regularization term $[2 lambda * w_{f,l}]$. . The update step looks like this: . $% &lt;![CDATA[ W mathrel{-}= begin{bmatrix} alpha nabla (w_{0,0}) &amp; alpha nabla (w_{0,1}) &amp; cdots &amp; alpha nabla (w_{0,k}) alpha nabla (w_{1,0}) &amp; alpha nabla (w_{1,1}) &amp; cdots &amp; alpha nabla (w_{1,k}) vdots &amp; vdots &amp; cdots &amp; vdots alpha nabla (w_{n,0}) &amp; alpha nabla (w_{n,1}) &amp; cdots &amp; alpha nabla (w_{n,k}) end{bmatrix} %]]&gt;$ . lambda_param = 0.01 lr = 0.01 w_gradients = -torch.matmul(X.T, y_one_hot -A) / X.shape[0] + (2 * lambda_param * weights) b_gradients = -torch.mean(y_one_hot - A, axis=0) weights -= lr * w_gradients bias -= lr * b_gradients . Defining the Loss Function . We are going to use the cross-entropy loss function which takes the negative log-likelihood of the predicted probability assigned to the true label. . The cost function or loss function is how we determine the perfomance of the model at the end of each forward pass in the training process. . Note: Technically, the loss refers to the accuracy of a single observation prediction while the cost is the average loss of all m predictions. . $Cross :Entropy :Loss = - frac{1}{m} sum_{i=0}^{m} sum_{j=0}^{k} y_{i,j} cdot log (a_{i,j})$ . def cross_entropy_loss(y_one_hot, activations): return -torch.mean( torch.sum( y_one_hot * torch.log(activations), axis=1 ) ) . Multiply the one-hot labels by the log of the activations. It&#39;s important to note here that only one column per row in the product will be non-zero. | Get the row-sums (axis=1) of the tensor in step 1. | Return the negative mean of step 3. | Define The Regularization . L2 Regularization . Regularization refer to methods that try to prevent overfitting in machine learning models- we can usually tell we are overfitting the data when training loss keeps going down but validation loss increases. . L2 Regularization simply adds a term to the cost function intended to penalize model complexity. It looks like this: . $L2 :Regularization :Term := big |W big |^{2} = sum_{i=0}^{n} sum_{j=0}^{k} w_{i,j}^2$ . We square every parameter $w$ in the weight matrix and take the sum. This is were the idea of penalizing complexity comes from - the sum gets bigger as the magnitude of the individual parameters or the number of them grows. Adding this term to the loss calculation has the effect of encouraging the model parameters towards zero. . Note: In general, we want to pre-process the features in your dataset to be of the same scale, but this is especially important when using L2 regularization. . $L2 :Regularized :Loss = - frac{1}{m} sum_{i=0}^{m} sum_{j=0}^{k} y_{i,j} cdot log (a_{i,j}) + lambda sum_{i=0}^{n} sum_{j=0}^{k} w_{i,j}^2$ . Where $λ≥0$ is a hyperparameter to be tuned and $λ=0$ is equivalent to vanilla Cross Entropy Loss. . l2_regularization = torch.sum(weights ** 2) lambda_param = 0.01 loss = cross_entropy_loss(y_one_hot, A) + lambda_param * l2_regularization . Classification Accuracy . test_predictions = torch.argmax(softmax(torch.mm(valid_x, weights) + bias), axis=1) . def accuracy_metric(actual, predicted): correct = 0 for i in range(len(actual)): if actual[i] == predicted[i]: correct += 1 return correct / float(len(actual)) * 100.0 . accuracy_metric(valid_y, test_predictions) . 14.799999999999999 . Training . n_train = 60000 . n_iterations = 100 learning_rate = 0.01 lambda_param = 0.01 for i in range(1, n_iterations + 1): Z = torch.mm(X, weights) + bias A = softmax(Z) l2_regularization = torch.sum(weights ** 2) loss = cross_entropy_loss(y_one_hot, A) + lambda_param * l2_regularization w_gradients = -torch.mm(X.transpose(0, 1), y_one_hot - A) / n_train + (2 * lambda_param * weights) b_gradients = -torch.mean(y_one_hot - A, axis=0) weights -= learning_rate * w_gradients bias -= learning_rate * b_gradients if i == 1 or i % 25 == 0: print(&quot;Loss at iteration {}: {}&quot;.format(i, loss)) test_predictions = torch.argmax(softmax(torch.mm(valid_x, weights) + bias), axis=1) test_accuracy = accuracy_metric(valid_y, test_predictions) print(&quot; nFinal Test Accuracy: {}&quot;.format(test_accuracy)) . Loss at iteration 1: 85.1719970703125 Loss at iteration 25: 83.82386779785156 Loss at iteration 50: 82.50579833984375 Loss at iteration 75: 81.2599868774414 Loss at iteration 100: 80.07752227783203 Final Test Accuracy: 17.91 . Z = torch.mm(X, weights) + bias A = softmax(Z) l2_regularization = torch.sum(weights ** 2) loss = cross_entropy_loss(y_one_hot, A) loss . Z = torch.matmul(train_x, weights) + bias Z . All PyTorch Tensors have a requires_grad attribute that defaults to False. If we set this property to True when the tensor gets created, PyTorch will keep track of every computation we perform with it as a graph. Calling backward() on any tensor that descends from the original will propagate gradients all the way back up the graph, populating the Tensor.grad attribute for all parent tensors. . weights = torch.randn((784, 10), requires_grad=True) bias = torch.randn(10, requires_grad=True) . n_iterations = 250 learning_rate = 0.1 lambda_param = 0.01 for i in range(1, n_iterations + 1): Z = torch.mm(X, weights) + bias A = softmax(Z) l2_regularization = torch.sum(weights ** 2) loss = cross_entropy_loss(y_one_hot, A) + lambda_param * l2_regularization if weights.grad is not None: weights.grad.zero_() # 1 if bias.grad is not None: bias.grad.zero_() loss.backward() # 2 with torch.no_grad(): # 3 weights -= learning_rate * weights.grad bias -= learning_rate * bias.grad if i == 1 or i % 25 == 0: print(&quot;Loss at iteration {}: {}&quot;.format(i, loss)) test_predictions = torch.argmax(softmax(torch.mm(valid_x, weights) + bias), axis=1) test_accuracy = accuracy_metric(valid_y, test_predictions) print(&quot; nFinal Test Accuracy: {}&quot;.format(test_accuracy)) . Loss at iteration 1: 29.02509117126465 Loss at iteration 25: 26.34192657470703 Loss at iteration 50: 23.82099151611328 Loss at iteration 75: 21.550569534301758 Loss at iteration 100: 19.504758834838867 Loss at iteration 125: 17.66069221496582 Loss at iteration 150: 15.998042106628418 Loss at iteration 175: 14.498688697814941 Loss at iteration 200: 13.146411895751953 Loss at iteration 225: 11.926678657531738 Loss at iteration 250: 10.826428413391113 Final Test Accuracy: 80.9 . Note: When we tell PyTorch to keep track of gradients they will keep acucmalating as a total sum every time we call backward(). We don&#39;t want this since we only ever want the gradients from the last step. Therefore we need to zero out the gradients for every tensor after we&#39;ve updated them with ´tensor.grad.zero_(). (PyTorch uses the convetion of ending method names with an underscore for in-place changes.) . Zero out the gradients before calculating them again. | Calculate upstream gradients. | In order to not disrupt the computation graph that we rely on for gradient calculations, we need to update both ´weights´ and ´bias´ inside of a no_grad context manager. |",
            "url": "https://sammm21.github.io/Sam_ds_blog/2021/06/15/SoftMax_Regression.html",
            "relUrl": "/2021/06/15/SoftMax_Regression.html",
            "date": " • Jun 15, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "FIRST BLOSS POST B... . Welcome to my Data Science Blog, B. . Since I want this to be a netfliggs blog and not bloggbusser blog I hope you will enjoy the code that I share, it will start out pretty basic but I hope to be able to share some cool projects over the summer. I&#39;m going to be mainly working in PyTorch were I will aim to build a range of different models from scratch and then later on implement some cool papers that I find interesting. . So without any further ado let&#39;s get into Deeeeeeep Learning, B. .",
            "url": "https://sammm21.github.io/Sam_ds_blog/2021/06/02/My-FirsS-PoSS-B.html",
            "relUrl": "/2021/06/02/My-FirsS-PoSS-B.html",
            "date": " • Jun 2, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://sammm21.github.io/Sam_ds_blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://sammm21.github.io/Sam_ds_blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://sammm21.github.io/Sam_ds_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://sammm21.github.io/Sam_ds_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}